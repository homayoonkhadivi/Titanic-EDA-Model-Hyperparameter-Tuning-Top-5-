![kaggle-fundamentals](https://user-images.githubusercontent.com/57557590/106743629-e8cc4b00-6633-11eb-9acf-b37c149b9506.png)

### Titanic - Machine Learning from Disaster 
(https://www.kaggle.com/c/titanic/overview)


# The Challenge:
The sinking of the Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.  While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.  In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).
# Target:
In this Kaggle Competition, We want to predict if the passengers survived or not.

# Content of The Titanic Exploratory Data Analysis

### Chapter-1 Data Load and Check
1-Outlier Detection

2-Joining Test and Train Data

3-Feature Check

### Chapter-2 Data Analysis
1-Feature Analysis

2-Correlation Between Feature

### Chapter-3 Missing Value
1-Find Missing Value

2-Fill Missing Value

### Chapter-4 Data Engineering
1-New Feature

2-Drop Feature

3-One Hot Encoding

### Chapter-5 Modeling
1-Train-Test Split

2-Classification Methods

3-Ensemble Modeling

4-Result

![Survived](https://user-images.githubusercontent.com/57557590/106743398-9559fd00-6633-11eb-9239-b8d9b8d1dce3.PNG)

![Corr](https://user-images.githubusercontent.com/57557590/106743403-968b2a00-6633-11eb-9b26-6528bf5d1b40.PNG)

### The results of Machine Learning Algorithms could be described as bellow:
### Logistic Regression 81%

### Random Forest Regression 81%

### Support Vector Machine (SVM) 82%

### K-Nearest Neighbors (KNN) 82%

### Ensemble Modeling 82%

# Results:
By performing some innovative Statistical Analytics, Data Preprocessing, and Feature Engineer, I could improve my performance in this competition from top 68% to top 5% which is a significant achievement in my Data Science Journey.

![Kaggle Top 5%](https://user-images.githubusercontent.com/57557590/106751103-043c5380-663e-11eb-9eca-173b169cf2a8.PNG)

